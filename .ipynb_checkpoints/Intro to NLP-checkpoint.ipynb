{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello,\n",
      "I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
      "\n",
      "\n",
      "\n",
      "Hello,\n",
      "I am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam = \"\"\"\n",
    "Hello,\\nI saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
    "\"\"\"\n",
    "\n",
    "ham = \"\"\"\n",
    "Hello,\\nI am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.\n",
    "\"\"\"\n",
    "print spam\n",
    "print\n",
    "print ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello,',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'your',\n",
       " 'contact',\n",
       " 'information',\n",
       " 'on',\n",
       " 'linkedin.',\n",
       " 'i',\n",
       " 'have',\n",
       " 'carefully',\n",
       " 'read',\n",
       " 'through',\n",
       " 'your',\n",
       " 'profile',\n",
       " 'and',\n",
       " 'you',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'an',\n",
       " 'outstanding',\n",
       " 'personality.',\n",
       " 'this',\n",
       " 'is',\n",
       " 'one',\n",
       " 'major',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'i',\n",
       " 'am',\n",
       " 'in',\n",
       " 'contact',\n",
       " 'with',\n",
       " 'you.',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'mr.',\n",
       " 'valery',\n",
       " 'grayfer',\n",
       " 'chairman',\n",
       " 'of',\n",
       " 'the',\n",
       " 'board',\n",
       " 'of',\n",
       " 'directors',\n",
       " 'of',\n",
       " 'pjsc',\n",
       " '\"lukoil\".',\n",
       " 'i',\n",
       " 'am',\n",
       " '86',\n",
       " 'years',\n",
       " 'old',\n",
       " 'and',\n",
       " 'i',\n",
       " 'was',\n",
       " 'diagnosed',\n",
       " 'with',\n",
       " 'cancer',\n",
       " '2',\n",
       " 'years',\n",
       " 'ago.',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'going',\n",
       " 'in',\n",
       " 'for',\n",
       " 'an',\n",
       " 'operation',\n",
       " 'later',\n",
       " 'this',\n",
       " 'week.',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'will/donate',\n",
       " 'the',\n",
       " 'sum',\n",
       " 'of',\n",
       " '8,750,000.00',\n",
       " 'euros(eight',\n",
       " 'million',\n",
       " 'seven',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'fifty',\n",
       " 'thousand',\n",
       " 'euros',\n",
       " 'only',\n",
       " 'etc.',\n",
       " 'etc.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 7, 'of': 4, 'and': 3, 'is': 2, 'etc.': 2, 'am': 2, 'an': 2, 'have': 2, 'in': 2, 'your': 2, 'to': 2, 'years': 2, 'with': 2, 'this': 2, 'contact': 2, 'the': 2, 'major': 1, 'old': 1, 'cancer': 1, 'outstanding': 1, 'seven': 1, 'decided': 1, 'through': 1, 'carefully': 1, 'euros(eight': 1, 'seem': 1, 'saw': 1, 'information': 1, 'for': 1, 'euros': 1, 'fifty': 1, '86': 1, 'sum': 1, '\"lukoil\".': 1, 'only': 1, 'pjsc': 1, 'mr.': 1, '2': 1, 'linkedin.': 1, 'will/donate': 1, 'you': 1, 'hundred': 1, 'was': 1, 'personality.': 1, 'chairman': 1, 'profile': 1, 'you.': 1, 'hello,': 1, 'ago.': 1, 'read': 1, 'going': 1, 'thousand': 1, 'million': 1, 'grayfer': 1, 'reason': 1, 'be': 1, 'one': 1, 'why': 1, 'on': 1, 'name': 1, 'week.': 1, '8,750,000.00': 1, 'later': 1, 'board': 1, 'operation': 1, 'will': 1, 'directors': 1, 'diagnosed': 1, 'valery': 1, 'my': 1})\n"
     ]
    }
   ],
   "source": [
    "print Counter(spam.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'to': 5, 'you': 4, 'of': 4, 'the': 3, 'and': 2, 'we': 2, 'scientist': 2, 'data': 2, 'i': 2, 'further': 2, 'this': 1, 'regards.': 1, 'find': 1, 'information': 1, 'am': 1, 'an': 1, 'at': 1, 'in': 1, 'our': 1, 'message': 1, 'pleased': 1, 'best': 1, 'if': 1, 'will': 1, 'would': 1, 'with': 1, 'interviews': 1, 'please': 1, 'writing': 1, 'application': 1, 'mr.': 1, 'location': 1, 'passed': 1, 'interview': 1, 'for': 1, 'john': 1, 'date,': 1, 'be': 1, 'hello,': 1, 'x.': 1, 'invite': 1, 'that': 1, 'any': 1, 'interview.': 1, 'regards': 1, 'let': 1, 'know': 1, 'hooli': 1, 'on-site': 1, 'me': 1, 'on': 1, 'your': 1, 'like': 1, 'assistance.': 1, 'attached': 1, 'senior': 1, 'inform': 1, 'smith.': 1, 'can': 1, 'time': 1, 'position': 1, 'first': 1, 'round': 1, 'are': 1})\n"
     ]
    }
   ],
   "source": [
    "print Counter(ham.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec = CountVectorizer()\n",
    "cvec.fit([spam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1,\n",
       "         1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2,\n",
       "         2, 2, 2]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## matrix with the counts of different words\n",
    "cvec.transform([spam]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df  = pd.DataFrame(cvec.transform([spam]).todense(),\n",
    "             columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>750</th>\n",
       "      <th>86</th>\n",
       "      <th>ago</th>\n",
       "      <th>am</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>be</th>\n",
       "      <th>board</th>\n",
       "      <th>...</th>\n",
       "      <th>to</th>\n",
       "      <th>valery</th>\n",
       "      <th>was</th>\n",
       "      <th>week</th>\n",
       "      <th>why</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>years</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  750  86  ago  am  an  and  be  board  ...   to  valery  was  week  \\\n",
       "0   1    1    1   1    1   2   2    3   1      1  ...    2       1    1     1   \n",
       "\n",
       "   why  will  with  years  you  your  \n",
       "0    1     2     2      2    2     2  \n",
       "\n",
       "[1 rows x 69 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>of</th>\n",
       "      <th>and</th>\n",
       "      <th>your</th>\n",
       "      <th>contact</th>\n",
       "      <th>is</th>\n",
       "      <th>in</th>\n",
       "      <th>have</th>\n",
       "      <th>euros</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   of  and  your  contact  is  in  have  euros  the  this\n",
       "0   4    3     2        2   2   2     2      2    2     2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## makes a data frame, adds the most frequent words in the e-mail\n",
    "\n",
    "df  = pd.DataFrame(cvec.transform([spam]).todense(),\n",
    "             columns=cvec.get_feature_names())\n",
    "\n",
    "df.transpose().sort_values(0, ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## doing this to both ham and spam\n",
    "\n",
    "full_vectorizer = CountVectorizer()\n",
    "full_vectorizer.fit([spam, ham])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>of</th>\n",
       "      <th>and</th>\n",
       "      <th>your</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>contact</th>\n",
       "      <th>have</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>euros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   of  and  your  in  is  contact  have  the  this  euros\n",
       "0   4    3     2   2   2        2     2    2     2      2\n",
       "1   4    2     1   1   0        0     0    3     1      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## makes the data frame for both ham and spam\n",
    "\n",
    "df  = pd.DataFrame(full_vectorizer.transform([spam, ham]).todense(),\n",
    "             columns=full_vectorizer.get_feature_names())\n",
    "\n",
    "df.transpose().sort_values(0, ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>479532</th>\n",
       "      <th>144749</th>\n",
       "      <th>174171</th>\n",
       "      <th>832412</th>\n",
       "      <th>828689</th>\n",
       "      <th>994433</th>\n",
       "      <th>1005907</th>\n",
       "      <th>170062</th>\n",
       "      <th>675997</th>\n",
       "      <th>959146</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.338062</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.084515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.331042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413803</td>\n",
       "      <td>0.331042</td>\n",
       "      <td>0.082761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082761</td>\n",
       "      <td>0.082761</td>\n",
       "      <td>0.082761</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    479532    144749    174171    832412    828689    994433    1005907  \\\n",
       "0  0.338062  0.169031  0.169031  0.169031  0.169031  0.169031  0.169031   \n",
       "1  0.331042  0.000000  0.413803  0.331042  0.082761  0.000000  0.082761   \n",
       "\n",
       "    170062    675997    959146   \n",
       "0  0.169031  0.169031  0.084515  \n",
       "1  0.082761  0.082761  0.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvec = HashingVectorizer()\n",
    "hvec.fit([spam, ham])\n",
    "\n",
    "df  = pd.DataFrame(hvec.transform([spam, ham]).todense())\n",
    "df.transpose().sort_values(0, ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I went to the zoo today.',\n",
       " ' What do you think of that?',\n",
       " ' I bet you hate it!']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_text = \"I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\"\n",
    "\n",
    "easy_split_text = [\"I went to the zoo today.\",\n",
    "                   \"What do you think of that?\",\n",
    "                   \"I bet you hate it!\",\n",
    "                   \"Or maybe you don't\"]\n",
    "\n",
    "def simple_sentencer(text):\n",
    "    '''take a string called `text` and return\n",
    "    a list of strings, each containing a sentence'''\n",
    "\n",
    "    sentences = []\n",
    "    substring = ''\n",
    "    for c in text:\n",
    "        if c in ('.', '!', '?'):\n",
    "            sentences.append(substring + c)\n",
    "            substring = ''\n",
    "        else:\n",
    "            substring += c\n",
    "    return sentences\n",
    "\n",
    "simple_sentencer(easy_text)\n",
    "\n",
    "## this is a good start, but it misses the last sentence, even though it is a valid sentence\n",
    "## people are bad at using grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I went to the zoo today.',\n",
       " 'What do you think of that?',\n",
       " 'I bet you hate it!',\n",
       " \"Or maybe you don't\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_detector = PunktSentenceTokenizer()\n",
    "sent_detector.sentences_from_text(easy_text)\n",
    "\n",
    "## this figures out the punctuation problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "here_is_my_text = ''' Normalization is when slightly different version of a word exist. For example: LinkedIn sees 6000+ variations of the title \"Software Engineer\" and 8000+ variations of the word \"IBM\".\n",
    "\n",
    "It would be wrong to consider the words \"MR.\" and \"mr\" to be different features, thus we need a technique to normalize words to a common root. This technique is called Stemming.\n",
    "\n",
    "Science, Scientist => Scien\n",
    "Swimming, Swimmer, Swim => Swim\n",
    "As we did above we could define a Stemmer based on rules:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Normalization is when slightly different version of a word exist. For example: LinkedIn sees 6000+ variations of the title \"Software Engineer\" and 8000+ variations of the word \"IBM\".\\n\\nIt would be wrong to consider the words \"MR.\" and \"mr\" to be different features, thus we need a technique to normalize words to a common root. This technique is called Stemming.\\n\\nScience, Scientist => Scien\\nSwimming, Swimmer, Swim => Swim\\nAs we did above we could define a Stemmer based on rules:'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "here_is_my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "broken_text = sent_detector.sentences_from_text(here_is_my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Normalization is when slightly different version of a word exist.\n",
      "For example: LinkedIn sees 6000+ variations of the title \"Software Engineer\" and 8000+ variations of the word \"IBM\".\n",
      "It would be wrong to consider the words \"MR.\"\n",
      "and \"mr\" to be different features, thus we need a technique to normalize words to a common root.\n",
      "This technique is called Stemming.\n",
      "Science, Scientist => Scien\n",
      "Swimming, Swimmer, Swim => Swim\n",
      "As we did above we could define a Stemmer based on rules:\n"
     ]
    }
   ],
   "source": [
    "for i in broken_text:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For example: LinkedIn sees 6000+ variations of the title \"Software Engineer\" and 8000+ variations of the word \"IBM\".'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broken_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x11b0fde10>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_bigrams = ngrams(broken_text, 2)\n",
    "my_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ngram in my_bigrams:\n",
    "    print ngram, '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Normalization \n",
      "\n",
      "is is \n",
      "\n",
      "when when \n",
      "\n",
      "slightli slightly \n",
      "\n",
      "differ different \n",
      "\n",
      "version version \n",
      "\n",
      "of of \n",
      "\n",
      "a a \n",
      "\n",
      "word word \n",
      "\n",
      "exist. exist. \n",
      "\n",
      "For For \n",
      "\n",
      "example: example: \n",
      "\n",
      "LinkedIn LinkedIn \n",
      "\n",
      "see sees \n",
      "\n",
      "6000+ 6000+ \n",
      "\n",
      "variat variations \n",
      "\n",
      "of of \n",
      "\n",
      "the the \n",
      "\n",
      "titl title \n",
      "\n",
      "\"Softwar \"Software \n",
      "\n",
      "Engineer\" Engineer\" \n",
      "\n",
      "and and \n",
      "\n",
      "8000+ 8000+ \n",
      "\n",
      "variat variations \n",
      "\n",
      "of of \n",
      "\n",
      "the the \n",
      "\n",
      "word word \n",
      "\n",
      "\"IBM\". \"IBM\". \n",
      "\n",
      "It It \n",
      "\n",
      "would would \n",
      "\n",
      "be be \n",
      "\n",
      "wrong wrong \n",
      "\n",
      "to to \n",
      "\n",
      "consid consider \n",
      "\n",
      "the the \n",
      "\n",
      "word words \n",
      "\n",
      "\"MR.\" \"MR.\" \n",
      "\n",
      "and and \n",
      "\n",
      "\"mr\" \"mr\" \n",
      "\n",
      "to to \n",
      "\n",
      "be be \n",
      "\n",
      "differ different \n",
      "\n",
      "features, features, \n",
      "\n",
      "thu thus \n",
      "\n",
      "we we \n",
      "\n",
      "need need \n",
      "\n",
      "a a \n",
      "\n",
      "techniqu technique \n",
      "\n",
      "to to \n",
      "\n",
      "normal normalize \n",
      "\n",
      "word words \n",
      "\n",
      "to to \n",
      "\n",
      "a a \n",
      "\n",
      "common common \n",
      "\n",
      "root. root. \n",
      "\n",
      "Thi This \n",
      "\n",
      "techniqu technique \n",
      "\n",
      "is is \n",
      "\n",
      "call called \n",
      "\n",
      "Stemming. Stemming. \n",
      "\n",
      "Science, Science, \n",
      "\n",
      "Scientist Scientist \n",
      "\n",
      "=> => \n",
      "\n",
      "Scien Scien \n",
      "\n",
      "Swimming, Swimming, \n",
      "\n",
      "Swimmer, Swimmer, \n",
      "\n",
      "Swim Swim \n",
      "\n",
      "=> => \n",
      "\n",
      "Swim Swim \n",
      "\n",
      "As As \n",
      "\n",
      "we we \n",
      "\n",
      "did did \n",
      "\n",
      "abov above \n",
      "\n",
      "we we \n",
      "\n",
      "could could \n",
      "\n",
      "defin define \n",
      "\n",
      "a a \n",
      "\n",
      "Stemmer Stemmer \n",
      "\n",
      "base based \n",
      "\n",
      "on on \n",
      "\n",
      "rules: rules: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in here_is_my_text.split():\n",
    "    print stemmer.stem(word), word, '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
